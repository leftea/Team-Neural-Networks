{
 "metadata": {
  "name": "",
  "signature": "sha256:fc10ae7997881c7e2e931476df59803d7865d71e5cc31be672e907f43d847d3f"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Handwritten Digit Recognizer"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Outline:\n",
      "0. Description of the problem\n",
      "1. Exploratory Analysis\n",
      "2. Image Processing Considerations\n",
      "3. Models\n",
      "    -KNN\n",
      "    -Multilayer perceptron\n",
      "    -Convolutional Neural Network\n",
      "4. Actual handwritten digits"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "images =pd.read_csv('digitTrain.csv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Description of the problem"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Exploratory Analysis"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Since this is a famous data set, it's fair to assume that no direct cleaning is needed, unless we would like to clean our data from an image processing perspective. \n",
      "\n",
      "First, let's look at what some of the images look like.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "labels = images['label']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "reshaped = [images.drop('label', axis = 1).values[i].reshape(28,28) for i in range(1,20)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib.cm as cm\n",
      "fig, axes= plt.subplots(1,4)\n",
      "axes[0].imshow(reshaped[1], cmap = cm.gist_yarg);\n",
      "axes[1].imshow(reshaped[2], cmap = cm.gist_yarg);\n",
      "axes[2].imshow(reshaped[3], cmap = cm.gist_yarg);\n",
      "axes[3].imshow(reshaped[10], cmap = cm.gist_yarg);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAWwAAABoCAYAAADVecobAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
       "text": [
        "<matplotlib.figure.Figure at 0x21f44470>"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can see that there is a fair bit of variation in the style of writing, as we would expect."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Image Processing considerations"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "dataList[0][1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "[1]"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "NN = MLP_Classifier(784, 4, 1, iterations = 50, learning_rate = 0.01, \n",
      "                        momentum = 0.5, rate_decay = 0.0001, \n",
      "                        output_layer = 'softmax')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "xdata = images.drop('label',axis = 1)\n",
      "dataList = []\n",
      "for i in range(len(images)):\n",
      "    tupledata = [xdata.values[i,:], [images.values[i,0]]]\n",
      "    dataList.append(tupledata)\n",
      "    \n",
      "NN.fit(dataList)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Using softmax activation in output layer\n",
        "Training error 0.00000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Training error 0.00000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Training error 0.00000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Training error 0.00000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Training error 0.00000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import time\n",
      "import random\n",
      "import numpy as np\n",
      "np.seterr(all = 'ignore')\n",
      "\n",
      "# transfer functions\n",
      "def sigmoid(x):\n",
      "    return 1 / (1 + np.exp(-x))\n",
      "\n",
      "# derivative of sigmoid\n",
      "def dsigmoid(y):\n",
      "    return y * (1.0 - y)\n",
      "\n",
      "# using softmax as output layer is recommended for classification where outputs are mutually exclusive\n",
      "def softmax(w):\n",
      "    e = np.exp(w - np.amax(w))\n",
      "    dist = e / np.sum(e)\n",
      "    return dist\n",
      "\n",
      "# using tanh over logistic sigmoid for the hidden layer is recommended   \n",
      "def tanh(x):\n",
      "    return np.tanh(x)\n",
      "    \n",
      "# derivative for tanh sigmoid\n",
      "def dtanh(y):\n",
      "    return 1 - y*y\n",
      "\n",
      "class MLP_Classifier(object):\n",
      "    \"\"\"\n",
      "    Basic MultiLayer Perceptron (MLP) neural network with regularization and learning rate decay\n",
      "    Consists of three layers: input, hidden and output. The sizes of input and output must match data\n",
      "    the size of hidden is user defined when initializing the network.\n",
      "    The algorithm can be used on any dataset.\n",
      "    As long as the data is in this format: [[[x1, x2, x3, ..., xn], [y1, y2, ..., yn]],\n",
      "                                           [[[x1, x2, x3, ..., xn], [y1, y2, ..., yn]],\n",
      "                                           ...\n",
      "                                           [[[x1, x2, x3, ..., xn], [y1, y2, ..., yn]]]\n",
      "    An example is provided below with the digit recognition dataset provided by sklearn\n",
      "    Fully pypy compatible.\n",
      "    \"\"\"\n",
      "    def __init__(self, input, hidden, output, iterations = 50, learning_rate = 0.01, \n",
      "                l2_in = 0, l2_out = 0, momentum = 0, rate_decay = 0, \n",
      "                output_layer = 'logistic', verbose = True):\n",
      "        \"\"\"\n",
      "        :param input: number of input neurons\n",
      "        :param hidden: number of hidden neurons\n",
      "        :param output: number of output neurons\n",
      "        :param iterations: how many epochs\n",
      "        :param learning_rate: initial learning rate\n",
      "        :param l2: L2 regularization term\n",
      "        :param momentum: momentum\n",
      "        :param rate_decay: how much to decrease learning rate by on each iteration (epoch)\n",
      "        :param output_layer: activation (transfer) function of the output layer\n",
      "        :param verbose: whether to spit out error rates while training\n",
      "        \"\"\"\n",
      "        # initialize parameters\n",
      "        self.iterations = iterations\n",
      "        self.learning_rate = learning_rate\n",
      "        self.l2_in = l2_in\n",
      "        self.l2_out = l2_out\n",
      "        self.momentum = momentum\n",
      "        self.rate_decay = rate_decay\n",
      "        self.verbose = verbose\n",
      "        self.output_activation = output_layer\n",
      "        \n",
      "        # initialize arrays\n",
      "        self.input = input + 1 # add 1 for bias node\n",
      "        self.hidden = hidden \n",
      "        self.output = output\n",
      "\n",
      "        # set up array of 1s for activations\n",
      "        self.ai = np.ones(self.input)\n",
      "        self.ah = np.ones(self.hidden)\n",
      "        self.ao = np.ones(self.output)\n",
      "\n",
      "        # create randomized weights\n",
      "        # use scheme from Efficient Backprop by LeCun 1998 to initialize weights for hidden layer\n",
      "        input_range = 1.0 / self.input ** (1/2)\n",
      "        self.wi = np.random.normal(loc = 0, scale = input_range, size = (self.input, self.hidden))\n",
      "        self.wo = np.random.uniform(size = (self.hidden, self.output)) / np.sqrt(self.hidden)\n",
      "        \n",
      "        # create arrays of 0 for changes\n",
      "        # this is essentially an array of temporary values that gets updated at each iteration\n",
      "        # based on how much the weights need to change in the following iteration\n",
      "        self.ci = np.zeros((self.input, self.hidden))\n",
      "        self.co = np.zeros((self.hidden, self.output))\n",
      "\n",
      "    def feedForward(self, inputs):\n",
      "        \"\"\"\n",
      "        The feedforward algorithm loops over all the nodes in the hidden layer and\n",
      "        adds together all the outputs from the input layer * their weights\n",
      "        the output of each node is the sigmoid function of the sum of all inputs\n",
      "        which is then passed on to the next layer.\n",
      "        :param inputs: input data\n",
      "        :return: updated activation output vector\n",
      "        \"\"\"\n",
      "        if len(inputs) != self.input-1:\n",
      "            raise ValueError('Wrong number of inputs you silly goose!')\n",
      "\n",
      "        # input activations\n",
      "        self.ai[0:self.input -1] = inputs\n",
      "\n",
      "        # hidden activations\n",
      "        sum = np.dot(self.wi.T, self.ai)\n",
      "        self.ah = tanh(sum)\n",
      "        \n",
      "        # output activations\n",
      "        sum = np.dot(self.wo.T, self.ah)\n",
      "        if self.output_activation == 'logistic':\n",
      "            self.ao = sigmoid(sum)\n",
      "        elif self.output_activation == 'softmax':\n",
      "            self.ao = softmax(sum)\n",
      "        else:\n",
      "            raise ValueError('Choose a compatible output layer activation or check your spelling ;-p') \n",
      "        \n",
      "        \n",
      "        return self.ao\n",
      "\n",
      "    def backPropagate(self, targets):\n",
      "        \"\"\"\n",
      "        For the output layer\n",
      "        1. Calculates the difference between output value and target value\n",
      "        2. Get the derivative (slope) of the sigmoid function in order to determine how much the weights need to change\n",
      "        3. update the weights for every node based on the learning rate and sig derivative\n",
      "\n",
      "        For the hidden layer\n",
      "        1. calculate the sum of the strength of each output link multiplied by how much the target node has to change\n",
      "        2. get derivative to determine how much weights need to change\n",
      "        3. change the weights based on learning rate and derivative\n",
      "        :param targets: y values\n",
      "        :param N: learning rate\n",
      "        :return: updated weights\n",
      "        \"\"\"\n",
      "        if len(targets) != self.output:\n",
      "            raise ValueError('Wrong number of targets you silly goose!')\n",
      "\n",
      "        # calculate error terms for output\n",
      "        # the delta (theta) tell you which direction to change the weights\n",
      "        if self.output_activation == 'logistic':\n",
      "            output_deltas = dsigmoid(self.ao) * -(targets - self.ao)\n",
      "        elif self.output_activation == 'softmax':\n",
      "            output_deltas = -(targets - self.ao)\n",
      "        else:\n",
      "            raise ValueError('Choose a compatible output layer activation or check your spelling ;-p') \n",
      "        \n",
      "        # calculate error terms for hidden\n",
      "        # delta (theta) tells you which direction to change the weights\n",
      "        error = np.dot(self.wo, output_deltas)\n",
      "        hidden_deltas = dtanh(self.ah) * error\n",
      "        \n",
      "        # update the weights connecting hidden to output, change == partial derivative\n",
      "        change = output_deltas * np.reshape(self.ah, (self.ah.shape[0],1))\n",
      "        regularization = self.l2_out * self.wo\n",
      "        self.wo -= self.learning_rate * (change + regularization) + self.co * self.momentum \n",
      "        self.co = change \n",
      "\n",
      "        # update the weights connecting input to hidden, change == partial derivative\n",
      "        change = hidden_deltas * np.reshape(self.ai, (self.ai.shape[0], 1))\n",
      "        regularization = self.l2_in * self.wi\n",
      "        self.wi -= self.learning_rate * (change + regularization) + self.ci * self.momentum \n",
      "        self.ci = change\n",
      "\n",
      "        # calculate error\n",
      "        if self.output_activation == 'softmax':\n",
      "            error = -sum(targets * np.log(self.ao))\n",
      "        elif self.output_activation == 'logistic':\n",
      "            error = sum(0.5 * (targets - self.ao)**2)\n",
      "        \n",
      "        return error\n",
      "\n",
      "    def test(self, patterns):\n",
      "        \"\"\"\n",
      "        Currently this will print out the targets next to the predictions.\n",
      "        Not useful for actual ML, just for visual inspection.\n",
      "        \"\"\"\n",
      "        for p in patterns:\n",
      "            print(p[1], '->', self.feedForward(p[0]))\n",
      "\n",
      "    def fit(self, patterns):\n",
      "        if self.verbose == True:\n",
      "            if self.output_activation == 'softmax':\n",
      "                print 'Using softmax activation in output layer'\n",
      "            elif self.output_activation == 'logistic':\n",
      "                print 'Using logistic sigmoid activation in output layer'\n",
      "                \n",
      "        num_example = np.shape(patterns)[0]\n",
      "                \n",
      "        for i in range(self.iterations):\n",
      "            error = 0.0\n",
      "            random.shuffle(patterns)\n",
      "            for p in patterns:\n",
      "                inputs = p[0]\n",
      "                targets = p[1]\n",
      "                self.feedForward(inputs)\n",
      "                error += self.backPropagate(targets)\n",
      "                \n",
      "            with open('error.txt', 'a') as errorfile:\n",
      "                errorfile.write(str(error) + '\\n')\n",
      "                errorfile.close()\n",
      "                \n",
      "            if i % 10 == 0 and self.verbose == True:\n",
      "                error = error/num_example\n",
      "                print('Training error %-.5f' % error)\n",
      "                \n",
      "            # learning rate decay\n",
      "            self.learning_rate = self.learning_rate * (self.learning_rate / (self.learning_rate + (self.learning_rate * self.rate_decay)))\n",
      "                \n",
      "    def predict(self, X):\n",
      "        \"\"\"\n",
      "        return list of predictions after training algorithm\n",
      "        \"\"\"\n",
      "        predictions = []\n",
      "        for p in X:\n",
      "            predictions.append(self.feedForward(p))\n",
      "        return predictions\n",
      "\n",
      "def demo():\n",
      "    from sklearn.preprocessing import scale\n",
      "    \"\"\"\n",
      "    run NN demo on the digit recognition dataset from sklearn\n",
      "    \"\"\"\n",
      "    def load_data():\n",
      "        data = np.loadtxt('digitTrain.csv', delimiter = ',')\n",
      "\n",
      "        # first ten values are the one hot encoded y (target) values\n",
      "        y = data[:,0:10]\n",
      "        \n",
      "        data = data[:,10:] # x data\n",
      "        data = scale(data)\n",
      "        \n",
      "        out = []\n",
      "        #print data.shape\n",
      "\n",
      "        # populate the tuple list with the data\n",
      "        for i in range(data.shape[0]):\n",
      "            tupledata = list((data[i,:].tolist(), y[i].tolist())) # don't mind this variable name\n",
      "            out.append(tupledata)\n",
      "\n",
      "        return out\n",
      "    \n",
      "    start = time.time()\n",
      "    \n",
      "    X = load_data()\n",
      "\n",
      "    #print X[9] # make sure the data looks right\n",
      "\n",
      "    NN = MLP_Classifier(64, 4000, 10, iterations = 50, learning_rate = 0.01, \n",
      "                        momentum = 0.5, rate_decay = 0.0001, \n",
      "                        output_layer = 'logistic')\n",
      "\n",
      "    NN.fit(X)\n",
      "    \n",
      "    end = time.time()\n",
      "    print end - start\n",
      "    \n",
      "    #NN.test(X)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "from sklearn.cross_validation import train_test_split \n",
      "from nolearn.lasagne import NeuralNet, BatchIterator\n",
      "from lasagne import layers\n",
      "from lasagne.nonlinearities import softmax\n",
      "from lasagne.updates import momentum, nesterov_momentum, sgd, rmsprop\n",
      "import numpy as np\n",
      "from matplotlib import pyplot\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.utils import shuffle\n",
      "\n",
      "\n",
      "def plot_loss(net):\n",
      "    \"\"\"\n",
      "    Plot the training loss and validation loss versus epoch iterations with respect to \n",
      "    a trained neural network.\n",
      "    \"\"\"\n",
      "    train_loss = np.array([i[\"train_loss\"] for i in net.train_history_])\n",
      "    valid_loss = np.array([i[\"valid_loss\"] for i in net.train_history_])\n",
      "    pyplot.plot(train_loss, linewidth = 3, label = \"train\")\n",
      "    pyplot.plot(valid_loss, linewidth = 3, label = \"valid\")\n",
      "    pyplot.grid()\n",
      "    pyplot.legend()\n",
      "    pyplot.xlabel(\"epoch\")\n",
      "    pyplot.ylabel(\"loss\")\n",
      "    #pyplot.ylim(1e-3, 1e-2)\n",
      "    pyplot.yscale(\"log\")\n",
      "    pyplot.show()\n",
      "\n",
      "\n",
      "train_df = pd.read_csv('digitTrain.csv') \n",
      "\n",
      "\n",
      "train_label = train_df.values[:,0]\n",
      "train_data = train_df.values[:, 1:]\n",
      "print \"train:\", train_data.shape, train_label.shape\n",
      "\n",
      "\n",
      "train_data = train_data.astype(np.float)\n",
      "train_label = train_label.astype(np.int32)\n",
      "train_data, train_label = shuffle(train_data, train_label, random_state = 21)\n",
      "\n",
      "\n",
      "#train_data, valid_data, train_label, valid_label = train_test_split(train_data, train_label, test_size = 0.2, random_state = 21)\n",
      "\n",
      "fc_1hidden = NeuralNet(\n",
      "    layers = [  # three layers: one hidden layer\n",
      "        ('input', layers.InputLayer),\n",
      "        ('hidden', layers.DenseLayer),\n",
      "        ('dropout', layers.DropoutLayer),\n",
      "        ('output', layers.DenseLayer),\n",
      "        ],\n",
      "    # layer parameters:\n",
      "    input_shape = (None, 784),  # 28x28 input pixels per batch\n",
      "    hidden_num_units = 100,  # number of units in hidden layer\n",
      "    dropout_p = 0.25, # dropout probability\n",
      "    output_nonlinearity = softmax,  # output layer uses softmax function\n",
      "    output_num_units = 10,  # 10 labels\n",
      "\n",
      "    # optimization method:\n",
      "    #update = nesterov_momentum,\n",
      "    update = sgd,\n",
      "    update_learning_rate = 0.001,\n",
      "    #update_momentum = 0.9,\n",
      "\n",
      "    eval_size = 0.1,\n",
      "\n",
      "    # batch_iterator_train = BatchIterator(batch_size = 20),\n",
      "    # batch_iterator_test = BatchIterator(batch_size = 20),\n",
      "\n",
      "    max_epochs = 100,  # we want to train this many epochs\n",
      "    verbose = 1,\n",
      "    )\n",
      "\n",
      "fc_1hidden.fit(train_data, train_label)\n",
      "plot_loss(fc_1hidden)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "train: (42000L, 784L) (42000L,)\n",
        "  InputLayer        \t(None, 784)         \tproduces     784 outputs"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  DenseLayer        \t(None, 100)         \tproduces     100 outputs\n",
        "  DropoutLayer      \t(None, 100)         \tproduces     100 outputs\n",
        "  DenseLayer        \t(None, 10)          \tproduces      10 outputs\n",
        "\n",
        " Epoch  |  Train loss  |  Valid loss  |  Train / Val  |  Valid acc  |  Dur\n",
        "--------|--------------|--------------|---------------|-------------|-------"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "     1  |  \u001b[94m  4.913953\u001b[0m  |  \u001b[32m  1.418835\u001b[0m  |     3.463372  |     63.74%  |  0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "     2  |  \u001b[94m  1.447152\u001b[0m  |  \u001b[32m  1.079770\u001b[0m  |     1.340241  |     73.46%  |  0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "     3  |  \u001b[94m  1.182551\u001b[0m  |  \u001b[32m  0.906699\u001b[0m  |     1.304237  |     77.10%  |  0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "     4  |  \u001b[94m  1.027075\u001b[0m  |  \u001b[32m  0.815295\u001b[0m  |     1.259758  |     79.51%  |  0.9s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "     5  |  \u001b[94m  0.933322\u001b[0m  |  \u001b[32m  0.742779\u001b[0m  |     1.256527  |     81.73%  |  0.9s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "     6  |  \u001b[94m  0.857034\u001b[0m  |  \u001b[32m  0.686894\u001b[0m  |     1.247694  |     83.37%  |  0.9s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "     7  |  \u001b[94m  0.798782\u001b[0m  |  \u001b[32m  0.638364\u001b[0m  |     1.251294  |     84.08%  |  0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "     8  |  \u001b[94m  0.750758\u001b[0m  |  \u001b[32m  0.617997\u001b[0m  |     1.214824  |     84.39%  |  0.9s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "     9  |  \u001b[94m  0.716030\u001b[0m  |  \u001b[32m  0.550294\u001b[0m  |     1.301177  |     85.78%  |  0.9s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    10  |  \u001b[94m  0.679090\u001b[0m  |  \u001b[32m  0.528475\u001b[0m  |     1.285001  |     86.04%  |  0.9s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    11  |  \u001b[94m  0.637861\u001b[0m  |  \u001b[32m  0.504922\u001b[0m  |     1.263286  |     86.78%  |  1.0s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    12  |  \u001b[94m  0.620076\u001b[0m  |    0.512554  |     1.209776  |     85.95%  |  0.9s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    13  |  \u001b[94m  0.595028\u001b[0m  |  \u001b[32m  0.480894\u001b[0m  |     1.237338  |     87.40%  |  0.9s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    14  |  \u001b[94m  0.571994\u001b[0m  |  \u001b[32m  0.456751\u001b[0m  |     1.252311  |     88.62%  |  0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    15  |  \u001b[94m  0.553133\u001b[0m  |  \u001b[32m  0.449696\u001b[0m  |     1.230017  |     88.43%  |  0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    16  |  \u001b[94m  0.539762\u001b[0m  |  \u001b[32m  0.441975\u001b[0m  |     1.221249  |     89.07%  |  0.9s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    17  |  \u001b[94m  0.520730\u001b[0m  |  \u001b[32m  0.440912\u001b[0m  |     1.181031  |     88.80%  |  0.9s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    18  |  \u001b[94m  0.511077\u001b[0m  |    0.442679  |     1.154511  |     89.07%  |  0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    19  |  \u001b[94m  0.495919\u001b[0m  |  \u001b[32m  0.431615\u001b[0m  |     1.148986  |     89.47%  |  0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    20  |  \u001b[94m  0.486572\u001b[0m  |  \u001b[32m  0.424377\u001b[0m  |     1.146556  |     89.77%  |  0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    21  |  \u001b[94m  0.476702\u001b[0m  |    0.435918  |     1.093560  |     89.30%  |  0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    22  |  \u001b[94m  0.463601\u001b[0m  |  \u001b[32m  0.414976\u001b[0m  |     1.117176  |     90.44%  |  0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    23  |  \u001b[94m  0.457188\u001b[0m  |  \u001b[32m  0.406195\u001b[0m  |     1.125537  |     90.58%  |  0.9s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    24  |  \u001b[94m  0.455021\u001b[0m  |    0.406423  |     1.119574  |     90.59%  |  0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    25  |  \u001b[94m  0.441662\u001b[0m  |    0.412700  |     1.070175  |     90.18%  |  0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    26  |  \u001b[94m  0.435070\u001b[0m  |  \u001b[32m  0.386928\u001b[0m  |     1.124420  |     91.06%  |  0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    27  |  \u001b[94m  0.424431\u001b[0m  |    0.388979  |     1.091140  |     91.01%  |  0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    28  |  \u001b[94m  0.422385\u001b[0m  |    0.399462  |     1.057386  |     90.54%  |  0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    29  |  \u001b[94m  0.414514\u001b[0m  |    0.391476  |     1.058847  |     91.26%  |  0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    30  |  \u001b[94m  0.408768\u001b[0m  |  \u001b[32m  0.385654\u001b[0m  |     1.059934  |     91.44%  |  0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    31  |  \u001b[94m  0.402662\u001b[0m  |  \u001b[32m  0.384181\u001b[0m  |     1.048106  |     91.30%  |  0.9s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    32  |  \u001b[94m  0.391978\u001b[0m  |  \u001b[32m  0.376424\u001b[0m  |     1.041319  |     91.94%  |  1.1s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    33  |  \u001b[94m  0.389140\u001b[0m  |  \u001b[32m  0.372513\u001b[0m  |     1.044632  |     91.59%  |  1.0s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    34  |  \u001b[94m  0.385943\u001b[0m  |  \u001b[32m  0.369226\u001b[0m  |     1.045275  |     91.94%  |  0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    35  |  \u001b[94m  0.374141\u001b[0m  |    0.370225  |     1.010577  |     91.74%  |  0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    36  |  \u001b[94m  0.371698\u001b[0m  |  \u001b[32m  0.367459\u001b[0m  |     1.011536  |     92.24%  |  0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    37  |  \u001b[94m  0.369297\u001b[0m  |  \u001b[32m  0.357388\u001b[0m  |     1.033323  |     92.36%  |  0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    38  |  \u001b[94m  0.365335\u001b[0m  |    0.365760  |     0.998836  |     92.07%  |  0.9s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    39  |  \u001b[94m  0.362463\u001b[0m  |  \u001b[32m  0.353088\u001b[0m  |     1.026553  |     92.52%  |  0.9s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    40  |  \u001b[94m  0.346551\u001b[0m  |    0.353434  |     0.980525  |     92.66%  |  0.9s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    41  |    0.348933  |  \u001b[32m  0.349754\u001b[0m  |     0.997655  |     92.83%  |  0.7s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    42  |    0.348037  |    0.356260  |     0.976921  |     92.71%  |  0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    43  |  \u001b[94m  0.338254\u001b[0m  |    0.363252  |     0.931182  |     92.35%  |  0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    44  |  \u001b[94m  0.333137\u001b[0m  |    0.361418  |     0.921748  |     92.54%  |  0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    45  |  \u001b[94m  0.330880\u001b[0m  |    0.353139  |     0.936967  |     92.83%  |  0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    46  |  \u001b[94m  0.330564\u001b[0m  |    0.355957  |     0.928664  |     92.85%  |  0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    47  |    0.332517  |    0.350723  |     0.948089  |     92.91%  |  1.3s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    48  |  \u001b[94m  0.330322\u001b[0m  |    0.351629  |     0.939407  |     92.56%  |  1.0s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    49  |  \u001b[94m  0.323585\u001b[0m  |    0.357390  |     0.905412  |     92.99%  |  1.0s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    50  |  \u001b[94m  0.320158\u001b[0m  |  \u001b[32m  0.341664\u001b[0m  |     0.937055  |     93.22%  |  1.0s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    51  |  \u001b[94m  0.317036\u001b[0m  |    0.358529  |     0.884269  |     92.80%  |  1.3s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    52  |  \u001b[94m  0.313580\u001b[0m  |    0.357584  |     0.876939  |     93.12%  |  0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    53  |    0.313833  |    0.351048  |     0.893990  |     93.38%  |  0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    54  |  \u001b[94m  0.306496\u001b[0m  |    0.362614  |     0.845241  |     92.78%  |  1.0s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    55  |    0.308221  |    0.353763  |     0.871264  |     93.12%  |  0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    56  |    0.310262  |    0.344778  |     0.899890  |     93.41%  |  0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    57  |  \u001b[94m  0.299512\u001b[0m  |    0.345794  |     0.866159  |     93.12%  |  0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    58  |    0.304435  |    0.349446  |     0.871191  |     93.28%  |  0.9s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    59  |  \u001b[94m  0.296843\u001b[0m  |    0.348166  |     0.852590  |     93.28%  |  1.0s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    60  |  \u001b[94m  0.295047\u001b[0m  |  \u001b[32m  0.341372\u001b[0m  |     0.864299  |     93.40%  |  0.9s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    61  |  \u001b[94m  0.292501\u001b[0m  |    0.363463  |     0.804760  |     93.45%  |  0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    62  |  \u001b[94m  0.282530\u001b[0m  |    0.352005  |     0.802632  |     93.36%  |  0.9s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    63  |    0.288234  |    0.344485  |     0.836709  |     93.26%  |  0.9s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    64  |    0.286895  |    0.348571  |     0.823061  |     93.43%  |  0.9s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    65  |    0.284311  |    0.363079  |     0.783054  |     93.12%  |  1.1s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    66  |    0.288470  |    0.344226  |     0.838024  |     93.57%  |  0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    67  |  \u001b[94m  0.279875\u001b[0m  |  \u001b[32m  0.340092\u001b[0m  |     0.822940  |     93.54%  |  0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    68  |  \u001b[94m  0.278065\u001b[0m  |    0.346089  |     0.803450  |     93.30%  |  0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    69  |  \u001b[94m  0.267933\u001b[0m  |    0.340788  |     0.786217  |     93.56%  |  0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    70  |    0.273710  |    0.350017  |     0.781992  |     93.28%  |  0.9s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    71  |    0.272293  |    0.349978  |     0.778029  |     93.57%  |  0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    72  |    0.268270  |    0.353057  |     0.759849  |     93.29%  |  0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    73  |    0.269018  |    0.349501  |     0.769720  |     93.50%  |  0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    74  |  \u001b[94m  0.264796\u001b[0m  |    0.347597  |     0.761790  |     93.58%  |  0.9s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    75  |    0.266451  |    0.348152  |     0.765327  |     93.26%  |  0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    76  |    0.265248  |    0.351349  |     0.754942  |     93.51%  |  0.7s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    77  |  \u001b[94m  0.264328\u001b[0m  |    0.347300  |     0.761093  |     93.47%  |  0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    78  |  \u001b[94m  0.261285\u001b[0m  |    0.342483  |     0.762914  |     93.67%  |  0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    79  |  \u001b[94m  0.256994\u001b[0m  |  \u001b[32m  0.335432\u001b[0m  |     0.766158  |     93.69%  |  0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    80  |    0.260857  |    0.339970  |     0.767294  |     93.76%  |  0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    81  |  \u001b[94m  0.252930\u001b[0m  |    0.340014  |     0.743881  |     93.86%  |  0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    82  |    0.259488  |    0.354321  |     0.732354  |     93.57%  |  0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    83  |    0.253045  |    0.349673  |     0.723662  |     93.71%  |  0.9s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    84  |  \u001b[94m  0.252374\u001b[0m  |    0.336321  |     0.750396  |     93.68%  |  0.9s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    85  |  \u001b[94m  0.250661\u001b[0m  |    0.338209  |     0.741143  |     93.88%  |  0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    86  |  \u001b[94m  0.247943\u001b[0m  |    0.348018  |     0.712445  |     93.69%  |  0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    87  |    0.250460  |    0.343611  |     0.728905  |     93.83%  |  0.9s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    88  |    0.248326  |    0.339788  |     0.730827  |     93.76%  |  0.9s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    89  |  \u001b[94m  0.244091\u001b[0m  |    0.339126  |     0.719765  |     93.90%  |  0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    90  |  \u001b[94m  0.241155\u001b[0m  |    0.347610  |     0.693752  |     93.79%  |  0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    91  |    0.243617  |    0.341335  |     0.713717  |     93.74%  |  0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    92  |  \u001b[94m  0.240376\u001b[0m  |    0.345758  |     0.695214  |     93.76%  |  0.9s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    93  |  \u001b[94m  0.239393\u001b[0m  |    0.337184  |     0.709978  |     94.02%  |  1.1s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    94  |    0.239804  |    0.339442  |     0.706465  |     93.93%  |  1.2s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    95  |    0.240853  |    0.340732  |     0.706869  |     93.74%  |  1.1s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    96  |  \u001b[94m  0.236991\u001b[0m  |    0.338312  |     0.700508  |     94.02%  |  0.9s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    97  |    0.238528  |    0.343140  |     0.695132  |     93.95%  |  0.9s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    98  |    0.238195  |    0.348107  |     0.684257  |     93.69%  |  0.9s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    99  |  \u001b[94m  0.235605\u001b[0m  |    0.336041  |     0.701120  |     93.90%  |  0.9s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "   100  |  \u001b[94m  0.232123\u001b[0m  |    0.341893  |     0.678934  |     93.90%  |  0.8s"
       ]
      }
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}