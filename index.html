<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Team-neural-networks by leftea</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='http://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Team-neural-networks</h1>
      <h2 class="project-tagline">Exploring Neural Network Theory, and Their Application to Computer Vision</h2>
      <a href="https://github.com/leftea/Team-Neural-Networks" class="btn">View on GitHub</a>
      <a href="https://github.com/leftea/Team-Neural-Networks/zipball/master" class="btn">Download .zip</a>
      <a href="https://github.com/leftea/Team-Neural-Networks/tarball/master" class="btn">Download .tar.gz</a>
    </section>

    <section class="main-content">
      <h1>
<a id="team-neural-networks" class="anchor" href="#team-neural-networks" aria-hidden="true"><span class="octicon octicon-link"></span></a>Team-Neural-Networks</h1>

<hr>

<p>Authors: Mitch &amp; Matt</p>

<p>The purpose of this repo is to provide an introduction to the theory of ANNs (Artifical Neural Networks) and demonstrate their applications on data. </p>

<h3>
<a id="presentation-plans" class="anchor" href="#presentation-plans" aria-hidden="true"><span class="octicon octicon-link"></span></a>Presentation plans:</h3>

<p><strong>1. Introduction:</strong>    </p>

<blockquote>
<ul>
<li>What are Artifical Neural Networks?<te/li>
<li>(A <em>very brief history</em> of ANNs ) - optional, but good to know regardless</li>
</ul>
</blockquote>

<p><strong>Network function:</strong>  </p>

<blockquote>
<ul>
<li>Outline the typical ANN parameters - interconnection pattern between different layers of neruons, learning process for updating weights of the interconnections, and the activation function that converts a neuron's weighted input to its output activation</li>
<li>Training Epoch - What is it, what does it mean vs iteration - An epoch is a measure of the number of times all of the training vectors are used once to update the weights.
<strong>Learning:</strong><br>
</li>
<li>Cost functions / Choosing a Cost function</li>
<li>Paradigms : In both of our datasets, we will likely rely on supervised learning techniques </li>
<li>Mean squared error</li>
</ul>
</blockquote>

<h3>
<a id="feedforward-neural-network" class="anchor" href="#feedforward-neural-network" aria-hidden="true"><span class="octicon octicon-link"></span></a>FeedForward Neural Network</h3>

<p><strong>Feed Forward:</strong>  </p>

<blockquote>
<ul>
<li>Connections between units do not form a directed cycle. Different from recurrent neural networks (RNNs).</li>
</ul>
</blockquote>

<p><strong>Single Layer Perceptron:</strong>  </p>

<blockquote>
<ul>
<li>Linear Classifier</li>
<li>Uses Linear predictor function combining set of weights with the feature vector</li>
<li>Online learning - process data in piece by piece fashion (ie in order that input is given, not having entire input avail from start)</li>
<li>Can reduce to simpler models given certain parameter</li>
</ul>
</blockquote>

<p><strong>Multilayer Perceptron:</strong>  </p>

<blockquote>
<ul>
<li>Multi-layers of computational units conected in feed-forward manner</li>
<li>Each neuron in a layer has direct connections to neurons in subsequent layer</li>
<li>Use of dropout layer to prevent overfitting</li>
<li>Sigmoid Function  (Common activation function)</li>
<li><strong>Back-propagation</strong></li>
<li>&gt;- Output values compared with correct answers to find value of some error function, and then the error is fed back through the network, and the weights of each connection are adjusted in order to reduce the value of the error function. Cycles of this eventually converge to a state where the error of the calculations is small. </li>
<li>Weight adjusting - general method for non-linear optimization is Gradient Descent, where the derivative of the error function with respect to the network weights is computed, and weights are changed such that error decreases.</li>
<li>Back-propagation can only be applied on ANNs with differentiable activation functions because of this</li>
</ul>
</blockquote>

<p><strong>Misc:</strong> </p>

<blockquote>
<ul>
<li>Additional techniques</li>
<li>Danger of overfitting the training data and not capturing true model of the data </li>
<li>To avoid overfitting one heuristic called early stopping can be used </li>
<li>dropout layer</li>
<li>Speed of convergence in back-propagation algorithms</li>
<li>Possibility of ending up in a local minimum of the error function</li>
</ul>
</blockquote>

<p><strong>Convolutional Neural Network:</strong>  </p>

<blockquote>
<ul>
<li>Feed-forward</li>
<li>Individual neurons tiled in an overlapping manner of regions in visual field</li>
<li>Inspired by biological processes / are variations of multilayer perceptron</li>
<li>During backpropagation momentum and weight decay are introduced, to avoid much oscillation during stochastic gradient descent</li>
<li>
<strong>CNN Layers:</strong><br>
</li>
<li>Convolutional Layer</li>
<li>&gt;- Parameters of each convolutional kernel are trained by backpropagation algorithm</li>
<li>&gt;- many convolution kernels in each layer </li>
<li>&gt;- each kernel is replicated over entire image with same parameters</li>
<li>&gt;- Function of the convolution operators is: Extract different features of the input</li>
<li>&gt;- First convolution layers will obtain low-level features such as edges, line curves, and the more layers there are, the more higher-level features it will get</li>
<li>ReLU Layer</li>
<li>&gt;- Rectified Linear Units</li>
<li>&gt;- Layer of neurons using non-saturating activation function f(x) = max(0,x) thereby increasing nonlinear properties of the decision function without affecting receptive fields of the convolution layer</li>
<li>&gt;- Other functions are used to increase nonlinearity such as hyperbolic tangent - f(x) - tanh(x), f(x) = |tanh(x)|, and the sigmoid function f(x) = (1+e^(-x))^(-1).</li>
<li>&gt;- Adv. of ReLU is that compared to these functions, neural network trains several times faster</li>
<li>Dropout "layer"</li>
<li>&gt;- Fully connected layer occupies most of the parameters and is prone to overfitting</li>
<li>&gt;- Dropout method introduced to prevent overfitting</li>
<li>&gt;- Also improves speed of training</li>
<li>&gt;- Dropout is performed randomly - in input layer, probability of dropping a neuron is between 0.5, 1 and in hidden layers, a probability of 0.5 is used. Neurons that get dropped do not contribute to the forward pass and back propagation. </li>
<li>Loss layer</li>
<li>&gt;- Loss functions could be used for different tasks</li>
<li>Softmax loss</li>
<li>&gt;- Predicting single class of K mutually exclusive classes</li>
<li>Sigmoid cross-entropy loss</li>
<li>&gt;- predicting K independent probability values in [0,1]</li>
<li>Euclidean loss</li>
<li>&gt;- Regressing to real-valued labels [-inf,inf]</li>
</ul>
</blockquote>

<h3>
<a id="problems-in-neural-networks----make-these-brief" class="anchor" href="#problems-in-neural-networks----make-these-brief" aria-hidden="true"><span class="octicon octicon-link"></span></a>Problems in Neural Networks</h3>

<p><strong>Association Rule Learning</strong>  </p>

<blockquote>
<ul>
<li>Find interesting rules and relations between variables in large databases</li>
</ul>
</blockquote>

<p><strong>Anomaly Detection</strong>  </p>

<blockquote>
<ul>
<li>Identification of items that do not conform to expected pattern </li>
<li>E.g. Intrusion detection, Fraud detection</li>
</ul>
</blockquote>

<p><strong>Grammar Induction</strong>  </p>

<blockquote>
<ul>
<li>Grammatical inference aka syntactic pattern recognition</li>
<li>Learning a formal grammar from a set of observations</li>
</ul>
</blockquote>

<p><strong>Classification and Regression problems</strong>  </p>

<blockquote>
<ul>
<li>Handwritten numbers (Classification of images into the numbers displayed)</li>
<li>Facial Keypoints (Regression, finding coordinates of key areas of faces in images)</li>
</ul>
</blockquote>

<h3>
<a id="applications" class="anchor" href="#applications" aria-hidden="true"><span class="octicon octicon-link"></span></a>Applications</h3>

<p><strong>Data processing</strong></p>

<p><strong>EDA</strong>
</p>

<p><strong>How we chose parameters</strong></p>

<p><strong>How many learning cycles did it take to reach a low error on the data (graphs)</strong></p>

<p><strong>Performance comparison of different neural networks we used (if we used more than one)</strong></p>

<p><strong>How we compared against others who did this problem on Kaggle</strong></p>


<h3>
<a id="tools" class="anchor" href="#tools" aria-hidden="true"><span class="octicon octicon-link"></span></a>Tools</h3>

<p><strong>Python</strong>  </p>

<blockquote>
<ul>
<li>Theano<br>
</li>
<li>Numpy<br>
</li>
<li>Lasagne</li>
<li>Nolearn</li>
<li>SKLearn</li>
<li>Pandas</li>
</ul>
</blockquote>

<h3>
<a id="remarks" class="anchor" href="#remarks" aria-hidden="true"><span class="octicon octicon-link"></span></a>Remarks</h3>

      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/leftea/Team-Neural-Networks">Team-neural-networks</a> is maintained by <a href="https://github.com/leftea">leftea</a>.</span>

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>

  
  </body>
</html>

